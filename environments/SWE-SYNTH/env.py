"""
SWE-SYNTH Environment (Evaluation Only)

Evaluates fixer models on pre-generated bug injection tasks.
Tasks must be pre-generated by the breaker service and stored in R2.

Flow:
    task_id → load from R2 → fixer repair → verify
"""

import os
import json
import time
import asyncio
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any
from datasets import load_dataset

# Import cache module
from cache import TwoLevelCache

# Import fixer agent abstraction
from fixer import create_fixer_agent, FixerConfig, AgentType


# Timeout constants (in seconds)
DOCKER_PULL_TIMEOUT = 300
FIXER_TIMEOUT = 1800
VERIFY_FIX_TIMEOUT = 1800


def get_dockerhub_image_uri(uid: str, dockerhub_username: str, repo_name: str) -> str:
    """Generate Docker Hub image URI matching SWE-bench naming scheme."""
    repo_base, repo_name_only = repo_name.lower().split("/")
    hsh = uid.replace("instance_", "")

    if uid == "instance_element-hq__element-web-ec0f940ef0e8e3b61078f145f34dc40d1938e6c5-vnan":
        repo_name_only = 'element-web'
    elif 'element-hq' in repo_name.lower() and 'element-web' in repo_name.lower():
        repo_name_only = 'element'
        if hsh.endswith('-vnan'):
            hsh = hsh[:-5]
    elif hsh.endswith('-vnan'):
        hsh = hsh[:-5]

    tag = f"{repo_base}.{repo_name_only}-{hsh}"
    if len(tag) > 128:
        tag = tag[:128]

    return f"{dockerhub_username}/sweap-images:{tag}"


class SynthActor:
    """
    SWE-SYNTH evaluation actor (fixer only).

    Evaluates fixer models on pre-generated tasks from R2.
    Tasks must be generated by the breaker service before evaluation.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        cache_dir: str = "/tmp/swe-synth-cache",
        dockerhub_username: str = "jefzda",
        dockerfiles_dir: str = "/app/ridges/dockerfiles",
        run_scripts_dir: str = "/app/ridges/run_scripts",
        # R2 read-only cache (public URL, no auth needed)
        r2_public_read_url: str = "https://pub-4b43a94ed07d4ac38fae3f4cb5070d6c.r2.dev",
        r2_prefix: str = "bugs",
    ):
        """
        Initialize SWE-SYNTH evaluation actor.

        Args:
            api_key: API key for LLM (optional, can also use environment variables)
            cache_dir: Directory for local cache
            dockerhub_username: Docker Hub username for images
            dockerfiles_dir: Path to SWE-bench dockerfiles
            run_scripts_dir: Path to SWE-bench run scripts
            r2_public_read_url: R2 public CDN URL for reading tasks
            r2_prefix: Prefix for R2 cache keys
        """
        self.api_key = api_key or os.getenv("CHUTES_API_KEY")
        self.dockerhub_username = dockerhub_username
        self.dockerfiles_dir = dockerfiles_dir
        self.run_scripts_dir = run_scripts_dir

        # Initialize two-level cache (read-only)
        self.cache = TwoLevelCache(
            local_cache_dir=cache_dir,
            r2_public_read_url=r2_public_read_url,
            r2_prefix=r2_prefix,
        )

        # Load SWE-bench Pro dataset (for verification)
        print("Loading SWE-bench Pro dataset...")
        dataset = load_dataset("ScaleAI/SWE-bench_Pro", split="test")
        sorted_instances = sorted(dataset, key=lambda x: x["instance_id"])
        self.swe_instances = {idx: inst for idx, inst in enumerate(sorted_instances)}
        print(f"Loaded {len(self.swe_instances)} SWE-bench Pro instances")

        # Cleanup stale containers from previous runs
        self._cleanup_stale_containers()

    def _cleanup_stale_containers(self):
        """Clean up stale ridge-proxy and ridges-sandbox containers from previous runs."""
        try:
            # Find and remove stale proxy containers
            result = subprocess.run(
                ["docker", "ps", "-aq", "--filter", "name=ridge-proxy-"],
                capture_output=True, text=True, timeout=30
            )
            if result.stdout.strip():
                containers = result.stdout.strip().split('\n')
                for cid in containers:
                    if cid:
                        subprocess.run(
                            ["docker", "rm", "-f", cid],
                            capture_output=True, timeout=30
                        )
                print(f"[SWE-SYNTH] Cleaned up {len(containers)} stale proxy containers")

            # Find and remove stale sandbox containers
            result = subprocess.run(
                ["docker", "ps", "-aq", "--filter", "name=ridges-sandbox-"],
                capture_output=True, text=True, timeout=30
            )
            if result.stdout.strip():
                containers = result.stdout.strip().split('\n')
                for cid in containers:
                    if cid:
                        subprocess.run(
                            ["docker", "rm", "-f", cid],
                            capture_output=True, timeout=30
                        )
                print(f"[SWE-SYNTH] Cleaned up {len(containers)} stale sandbox containers")

        except Exception as e:
            print(f"[SWE-SYNTH] Warning: Failed to cleanup stale containers: {e}")

    def _load_task(self, task_id: int) -> Dict[str, Any]:
        """
        Load pre-generated task from R2.

        Args:
            task_id: Task ID to load

        Returns:
            Task data dict (bug_instance)

        Raises:
            ValueError: If task not found in R2
        """
        bug_instance = self.cache.load(task_id)
        if bug_instance is None:
            raise ValueError(
                f"Task {task_id} not found. "
                f"Tasks must be pre-generated by the breaker service."
            )
        return bug_instance

    def _load_instance_script(self, instance_id: str, script_name: str) -> Optional[str]:
        """Load instance-specific script."""
        script_path = Path(self.run_scripts_dir) / instance_id / script_name
        if not script_path.exists():
            return None
        with open(script_path, 'r') as f:
            return f.read()

    def _load_dockerfile(self, instance_id: str, dockerfile_type: str) -> str:
        """Load Dockerfile content."""
        dockerfile_path = f"{self.dockerfiles_dir}/{dockerfile_type}_dockerfile/{instance_id}/Dockerfile"
        with open(dockerfile_path) as fp:
            return fp.read()

    def _extract_env_commands(self, base_dockerfile: str, instance_dockerfile: str) -> str:
        """Extract ENV commands from Dockerfiles."""
        env_cmds = []
        for dockerfile_content in [base_dockerfile, instance_dockerfile]:
            for line in dockerfile_content.split("\n"):
                line = line.strip()
                if line.startswith("ENV"):
                    env_cmd = line.replace("ENV", "export", 1)
                    env_cmds.append(env_cmd)
        return "\n".join(env_cmds)

    def _verify_fix(
        self,
        bug_instance: Dict[str, Any],
        fix_patch: str,
    ) -> tuple[float, Dict[str, Any]]:
        """
        Verify if the fix patch resolves the bug.

        Returns:
            Tuple of (score, test_stats)
        """
        if not fix_patch or not fix_patch.strip():
            return 0.0, {"error": "no patch"}

        try:
            source = bug_instance["source"]
            instance_id = source["swe_instance_id"]
            repo = source["repo"]
            base_commit = source["base_commit"]

            # Get original instance for test info
            swe_instance = None
            for inst in self.swe_instances.values():
                if inst["instance_id"] == instance_id:
                    swe_instance = inst
                    break

            if not swe_instance:
                return 0.0, {"error": f"Instance not found: {instance_id}"}

            # Get test requirements
            fail_to_pass = swe_instance.get("FAIL_TO_PASS", swe_instance.get("fail_to_pass", "[]"))
            pass_to_pass = swe_instance.get("PASS_TO_PASS", swe_instance.get("pass_to_pass", "[]"))

            if isinstance(fail_to_pass, str):
                try:
                    fail_to_pass = eval(fail_to_pass)
                except:
                    fail_to_pass = []
            if isinstance(pass_to_pass, str):
                try:
                    pass_to_pass = eval(pass_to_pass)
                except:
                    pass_to_pass = []

            f2p = set(fail_to_pass)
            p2p = set(pass_to_pass)
            required_tests = f2p | p2p

            if not required_tests:
                return 0.0, {"error": "No required tests"}

            # Load scripts
            run_script = self._load_instance_script(instance_id, "run_script.sh")
            parser_script = self._load_instance_script(instance_id, "parser.py")

            if not run_script or not parser_script:
                return 0.0, {"error": "Missing scripts"}

            # Build verification script
            try:
                base_dockerfile = self._load_dockerfile(instance_id, "base")
                instance_dockerfile = self._load_dockerfile(instance_id, "instance")
                env_cmds = self._extract_env_commands(base_dockerfile, instance_dockerfile)
            except Exception:
                env_cmds = ""

            before_repo_set_cmd = swe_instance.get("before_repo_set_cmd", "").strip()
            if before_repo_set_cmd:
                before_repo_set_cmd = before_repo_set_cmd.split("\n")[-1]

            selected_test_files = swe_instance.get("selected_test_files_to_run", "[]")
            if isinstance(selected_test_files, str):
                try:
                    selected_test_files = eval(selected_test_files)
                except:
                    selected_test_files = []
            test_files_str = ",".join(selected_test_files) if selected_test_files else ""

            import base64

            # Get patches
            gold_patch = bug_instance["original"]["gold_patch"]
            bug_patch = bug_instance["bug"]["patch"]

            # Flow: base_commit → gold_patch → bug_patch → fix_patch
            entryscript = f"""
{env_cmds}
cd /app
git reset --hard {base_commit}
git checkout {base_commit}

# Step 1: Apply gold_patch to get correct code (all tests pass)
git apply -v /workspace/gold_patch.diff

# Step 2: Apply bug_patch to inject the bug (target tests should fail)
git apply -v /workspace/bug_patch.diff || true

# Step 3: Apply fix_patch from Fixer (should restore correctness)
git apply -v /workspace/fix_patch.diff
{before_repo_set_cmd}

# Run tests
bash /workspace/run_script.sh {test_files_str} > /workspace/stdout.log 2> /workspace/stderr.log
python /workspace/parser.py /workspace/stdout.log /workspace/stderr.log /workspace/output.json
"""

            gold_patch_b64 = base64.b64encode(gold_patch.encode('utf-8')).decode('ascii')
            bug_patch_b64 = base64.b64encode(bug_patch.encode('utf-8')).decode('ascii')
            fix_patch_b64 = base64.b64encode(fix_patch.encode('utf-8')).decode('ascii')
            run_script_b64 = base64.b64encode(run_script.encode('utf-8')).decode('ascii')
            parser_script_b64 = base64.b64encode(parser_script.encode('utf-8')).decode('ascii')
            entryscript_b64 = base64.b64encode(entryscript.encode('utf-8')).decode('ascii')

            full_script = f"""#!/bin/bash
mkdir -p /workspace
echo "{gold_patch_b64}" | base64 -d > /workspace/gold_patch.diff
echo "{bug_patch_b64}" | base64 -d > /workspace/bug_patch.diff
echo "{fix_patch_b64}" | base64 -d > /workspace/fix_patch.diff
echo "{run_script_b64}" | base64 -d > /workspace/run_script.sh
echo "{parser_script_b64}" | base64 -d > /workspace/parser.py
echo "{entryscript_b64}" | base64 -d > /workspace/entryscript.sh
chmod +x /workspace/run_script.sh /workspace/entryscript.sh
bash /workspace/entryscript.sh
if [ -f /workspace/output.json ]; then
    echo "===SWESYNTH_OUTPUT_BEGIN==="
    cat /workspace/output.json
    echo "===SWESYNTH_OUTPUT_END==="
fi
"""

            # Get Docker image and run
            image = get_dockerhub_image_uri(instance_id, self.dockerhub_username, repo)

            print(f"Pulling image for verify_fix: {image}")
            subprocess.run(
                ["docker", "pull", image],
                check=False, capture_output=True, timeout=DOCKER_PULL_TIMEOUT
            )

            print(f"Running verification container (timeout={VERIFY_FIX_TIMEOUT}s)...")
            result = subprocess.run(
                ["docker", "run", "--rm", "-i", "--entrypoint", "/bin/bash", image],
                input=full_script,
                capture_output=True,
                timeout=VERIFY_FIX_TIMEOUT,
                text=True
            )
            print("Verification container completed.")

            stdout = result.stdout

            begin_marker = "===SWESYNTH_OUTPUT_BEGIN==="
            end_marker = "===SWESYNTH_OUTPUT_END==="

            if begin_marker not in stdout or end_marker not in stdout:
                return 0.0, {"error": "No output markers", "stderr": result.stderr[:500]}

            json_start = stdout.index(begin_marker) + len(begin_marker)
            json_end = stdout.index(end_marker)
            json_str = stdout[json_start:json_end].strip()

            output = json.loads(json_str)

            passed_tests = {x["name"] for x in output["tests"] if x["status"] == "PASSED"}

            # Get target tests from bug_instance (tests that Breaker tried to break)
            target_tests = set(bug_instance["bug"].get("target_tests", []))

            # Success criteria: target_tests must pass (bug was fixed)
            # Also check all tests for completeness
            all_required = f2p | p2p
            target_passed = target_tests <= passed_tests
            all_passed = all_required <= passed_tests

            target_passed_count = len(target_tests & passed_tests)
            total_target = len(target_tests)
            all_passed_count = len(all_required & passed_tests)
            total_all = len(all_required)

            test_stats = {
                "target_tests": list(target_tests),
                "target_result": f"{target_passed_count}/{total_target}",
                "all_result": f"{all_passed_count}/{total_all}",
                "target_passed": target_passed,
                "all_passed": all_passed,
            }

            # Score: 1 if all tests pass, 0 otherwise
            if all_passed:
                return 1.0, test_stats
            else:
                # Record which tests failed
                test_stats["missing_tests"] = list(all_required - passed_tests)
                return 0.0, test_stats

        except subprocess.TimeoutExpired:
            return 0.0, {"error": "timeout"}
        except Exception as e:
            import traceback
            return 0.0, {"error": traceback.format_exc()}

    async def evaluate(
        self,
        task_id: int,
        # Fixer model config
        model: str = "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8-TEE",
        base_url: str = "https://llm.chutes.ai/v1",
        api_key: Optional[str] = None,
        # Agent type selection
        fixer_agent: AgentType = "ridge",
        # Common execution config
        timeout: int = 1800,
        temperature: float = 0.0,
        seed: Optional[int] = None,
        max_iterations: int = 30,
        cost_limit: float = 10.0,
    ) -> Dict[str, Any]:
        """
        Evaluate a fixer model on a pre-generated task.

        Tasks must be pre-generated by the breaker service and stored in R2.

        Args:
            task_id: Task ID (must exist in R2)
            model: Model for fixing bugs
            base_url: API base URL
            api_key: API key (uses env var CHUTES_API_KEY if not provided)
            fixer_agent: Agent type for bug fixing ("miniswe", "ridge")
            timeout: Timeout for commands
            temperature: Model temperature for fixer
            seed: Random seed for LLM inference
            max_iterations: Max agent steps for fixer
            cost_limit: Max cost for fixer

        Returns:
            Result dict with score and metadata

        Raises:
            ValueError: If task not found in R2
        """
        start = time.time()

        # Use provided api_key or fall back to instance api_key
        fixer_api_key = api_key or self.api_key
        if not fixer_api_key:
            raise ValueError("api_key required (pass to evaluate() or set CHUTES_API_KEY env var)")

        # Load pre-generated task from R2
        bug_instance = self._load_task(task_id)
        print(f"Loaded task {task_id}: {bug_instance['source']['swe_instance_id']}")

        # Fix bug using selected agent
        print(f"Fixing bug with {model} using {fixer_agent} agent...")

        # Get Docker image for the bug instance
        source = bug_instance["source"]
        instance_id = source["swe_instance_id"]
        repo = source["repo"]
        docker_image = get_dockerhub_image_uri(instance_id, self.dockerhub_username, repo)

        # Create fixer config
        fixer_config = FixerConfig(
            model=model,
            api_base=base_url,
            api_key=fixer_api_key,
            temperature=temperature,
            max_iterations=max_iterations,
            cost_limit=cost_limit,
            timeout=timeout,
            seed=seed,
        )

        # Create and run fixer agent
        agent = create_fixer_agent(fixer_agent, fixer_config)
        problem_statement = bug_instance["bug"]["problem_statement"]

        # Get patches needed to prepare buggy code state
        gold_patch = bug_instance["original"].get("gold_patch", "")
        bug_patch = bug_instance["bug"].get("patch", "")
        base_commit = source.get("base_commit", "")

        try:
            result = await agent.fix(
                problem_statement=problem_statement,
                docker_image=docker_image,
                gold_patch=gold_patch,
                bug_patch=bug_patch,
                base_commit=base_commit,
            )
            fix_patch = result.patch
            fixer_metadata = {
                "model_calls": result.model_calls,
                "model_cost": result.model_cost,
            }
            if result.error:
                fixer_metadata["fixer_error"] = result.error
            conversation = result.conversation or []
            # Build usage dict (matching openspiel format)
            usage = {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": result.total_tokens,
            }
        finally:
            agent.cleanup()

        # Verify fix
        print("Verifying fix...")
        if not fix_patch or not fix_patch.strip():
            # Distinguish between infrastructure errors vs model failures
            # Following openspiel pattern:
            # - Infrastructure errors (timeout, API, docker): set error field (invalid sample, can retry)
            # - Model completed but no patch: no error field (valid sample, model just failed)
            if result.error:
                error_msg = result.error.lower()
                if "timeout" in error_msg or "timed out" in error_msg:
                    # Infrastructure timeout - invalid sample
                    test_stats = {"error": "fixer_timeout", "details": result.error}
                elif "api" in error_msg or "authentication" in error_msg or "connection" in error_msg or "network" in error_msg:
                    # API/network error - invalid sample
                    test_stats = {"error": "api_error", "details": result.error}
                elif "docker" in error_msg or "container" in error_msg:
                    # Docker infrastructure error - invalid sample
                    test_stats = {"error": "docker_error", "details": result.error}
                else:
                    # Other fixer errors that might be infrastructure related
                    test_stats = {"error": "fixer_error", "details": result.error}
            else:
                # Model completed execution but didn't generate a valid patch
                # This is a valid sample - model just couldn't solve the problem
                # No error field = valid sample with score 0
                test_stats = {"failure_reason": "no_patch_generated"}
            score = 0.0
        else:
            score, test_stats = self._verify_fix(bug_instance, fix_patch)

        # Get bug types from bug_instance
        bug_types = bug_instance.get("bug", {}).get("bug_types", [])

        result = {
            "task_name": "swe-synth",
            "score": score,
            "success": score > 0.0,
            "time_taken": time.time() - start,
            "extra": {
                # Task identification
                "task_id": task_id,
                "task_type": "swe-synth",
                "swe_instance_id": instance_id,
                "bug_types": bug_types,
                # Problem and solution
                "problem_statement": problem_statement,
                "fix_patch": fix_patch or "",
                # Interaction trajectory (matching openspiel format)
                "conversation": conversation,
                "usage": usage,
                # Fixer metadata
                **fixer_metadata,
                # Test results
                **test_stats,
            }
        }

        return result


# Alias for framework compatibility
Actor = SynthActor
